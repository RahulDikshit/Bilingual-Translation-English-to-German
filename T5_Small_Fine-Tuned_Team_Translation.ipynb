{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c3c656",
   "metadata": {},
   "source": [
    "Team Translation:\n",
    "Omkar Kadam\n",
    "Rahul Dikshit\n",
    "Sheethal Raghavendra\n",
    "Madhura Aravendekar\n",
    "Nitnada Manoonphol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1852e57-846c-44b4-b3d5-628d957ca68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: Trained_38/epoch_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: Trained_38/epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: Trained_38/epoch_2.pt\n",
      "Translated Sentence Greedy: Hallo, ich bin Omkar und das ist ein test.\n",
      "Translated Sentence Multinomial: Hallo, ich bin Omkar und das ist eine prüfung.\n",
      "Translated Sentence Beam: Hallo, ich bin Omkar und das ist ein test.\n",
      "Translated Sentence Beam Multinomial: Hallo, ich bin Omkar und das ist ein test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating translations: 100%|██████████| 10/10 [00:02<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 82.55759480385746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BERTScore:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577021e9b5bf4e55b1c05a6454541710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38fcbd39ecf40f99ec2356b970b86a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ec5a5e4c99469b9ab44e90577ec492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2ab385aa684254ada8cfbe778c2741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7b0868bfcc4926a760ff69628d375f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bac5196710741b5a9d18121733b9561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BERTScore: 100%|██████████| 1/1 [00:19<00:00, 19.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: 0.9860177397727966\n",
      "Scores:\n",
      "BLEU: 82.55759480385746\n",
      "BERT: 0.9860177397727966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set Up the Environment\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sacrebleu.metrics import BLEU\n",
    "from torch.nn import DataParallel\n",
    "from unidecode import unidecode\n",
    "from datetime import datetime\n",
    "from bert_score import score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import os\n",
    "import glob\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import DataParallel\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 2: Load and Preprocess the Dataset\n",
    "# Numbers of Rows before droping the null columns - 1920210\n",
    "# Numbers of Rows after droping the null - 1908920 (11,290)\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to read and preprocess JSONL file\n",
    "def read_jsonl(file_path):\n",
    "  data = []\n",
    "  with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "      try:\n",
    "        data.append(json.loads(line))\n",
    "      except json.JSONDecodeError as e:\n",
    "        print(f\"Skipping line due to JSONDecodeError: {e}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "jsonl_file_path = 'train_new_output.jsonl'  # Replace with the path to your JSONL file\n",
    "\n",
    "df = read_jsonl(jsonl_file_path)\n",
    "\n",
    "# Preprocess the DataFrame\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences, tokenizer, max_length):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sentence = self.source_sentences[idx]\n",
    "        target_sentence = self.target_sentences[idx]\n",
    "\n",
    "        source_sentence = source_sentence[:self.max_length]\n",
    "        target_sentence = target_sentence[:self.max_length]\n",
    "\n",
    "        input_text = f\"translate English to German: {source_sentence}\"\n",
    "        target_text = target_sentence\n",
    "\n",
    "        try:\n",
    "            input_ids = self.tokenizer.encode(input_text, add_special_tokens=True)\n",
    "            target_ids = self.tokenizer.encode(target_text, add_special_tokens=True)\n",
    "        except KeyError:\n",
    "            print(f\"Skipping example with index {idx} due to tokenization error.\")\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'target_ids': torch.tensor(target_ids),\n",
    "        }\n",
    "\n",
    "# Prepare the training and validation datasets\n",
    "train_size = int(0.8 * len(df))\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')  # Use a smaller model\n",
    "\n",
    "max_length = 256  # Reduce max length if appropriate\n",
    "\n",
    "# Create the translation datasets\n",
    "train_dataset = TranslationDataset(train_df['English'], train_df['German'], tokenizer, max_length)\n",
    "val_dataset = TranslationDataset(val_df['English'], val_df['German'], tokenizer, max_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16  # Increase batch size if GPU memory allows\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True)\n",
    "    target_ids = pad_sequence([item['target_ids'] for item in batch], batch_first=True)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'target_ids': target_ids\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "# Fine-tuning configuration\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Ensure a GPU is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise EnvironmentError(\"CUDA is not available. This script requires a GPU to run.\")\n",
    "\n",
    "# Create and initialize the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')  # Use a smaller model\n",
    "model.config.gradient_checkpointing = True\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Wrap the model with DataParallel if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Set the directory to save the model checkpoints\n",
    "checkpoint_dir = 'Trained_38'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "import os\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Assuming other parts of your code remain the same and only showing relevant parts for saving the model\n",
    "\n",
    "def save_model(model, tokenizer, directory):\n",
    "    \"\"\"\n",
    "    Saves the model and tokenizer to the specified directory.\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(directory, 'model')\n",
    "    model.save_pretrained(model_path)\n",
    "    \n",
    "    # Save the tokenizer\n",
    "    tokenizer_path = os.path.join(directory, 'tokenizer')\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "# Example usage after training and evaluation\n",
    "model_directory = 'Trained_T5_38'\n",
    "save_model(model, tokenizer, model_directory)\n",
    "\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='38_tensorboard_logs')\n",
    "\n",
    "# Fine-tuning loop\n",
    "accumulation_steps = 2  # Adjust if necessary\n",
    "\n",
    "# Initialize start_epoch in case no checkpoint is loaded\n",
    "start_epoch = 0\n",
    "\n",
    "# Load the last saved checkpoint if available\n",
    "best_val_loss = float('inf')\n",
    "existing_checkpoints = sorted(glob.glob(os.path.join(checkpoint_dir, 'epoch_*.pt')))\n",
    "if existing_checkpoints:\n",
    "    latest_checkpoint = existing_checkpoints[-1]\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['val_loss']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    print(f\"Loaded checkpoint: {latest_checkpoint} (Epoch {start_epoch}, Best Val Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "# Adjust epochs if resuming training\n",
    "epochs = start_epoch + epochs\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()  # For mixed precision\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    accumulation_counter = 0\n",
    "\n",
    "    train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}, Current epoch start: {current_time}\", leave=False)\n",
    "\n",
    "    for batch in train_progress:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # For mixed precision\n",
    "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        loss = loss / accumulation_steps\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        accumulation_counter += 1\n",
    "        if accumulation_counter % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        train_progress.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / (len(train_loader) * accumulation_steps)\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    val_progress = tqdm(val_loader, desc=f\"Validation {epoch+1}/{epochs}, Current epoch start: {current_time}\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_progress):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():  # For mixed precision\n",
    "                    outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_progress.set_postfix({'Loss': loss.item()})\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx} of the validation loop:\")\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'val_loss': val_loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }\n",
    "\n",
    "    checkpoint_filename = f'epoch_{epoch}.pt'\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "writer.close()\n",
    "def translate_sentence_greedy(sentence):\n",
    "    input_text = f\"translate English to German: {sentence}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    output = model.generate(input_ids=input_ids, num_beams=1, do_sample=False, max_length=50)\n",
    "    translated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "\n",
    "def translate_sentence_multinomial(sentence):\n",
    "    input_text = f\"translate English to German: {sentence}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    output = model.generate(input_ids=input_ids, num_beams=1, do_sample=True, max_length=50)\n",
    "    translated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "\n",
    "def translate_sentence_beam(sentence, n):\n",
    "    input_text = f\"translate English to German: {sentence}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    output = model.generate(input_ids=input_ids, num_beams=n, do_sample=False, max_length=50)\n",
    "    translated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "\n",
    "def translate_sentence_beam_multinomial(sentence, n):\n",
    "    input_text = f\"translate English to German: {sentence}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    output = model.generate(input_ids=input_ids, num_beams=n, do_sample=True, max_length=50)\n",
    "    translated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "source_sentence = \"Hello, I am Omkar and this is a test.\"\n",
    "\n",
    "translated_sentence_greedy = translate_sentence_greedy(source_sentence)\n",
    "translated_sentence_multinomial = translate_sentence_multinomial(source_sentence)\n",
    "translated_sentence_beam = translate_sentence_beam(source_sentence, 3)\n",
    "translated_sentence_beam_multinomial = translate_sentence_beam_multinomial(source_sentence, 3)\n",
    "\n",
    "print(f\"Translated Sentence Greedy: {translated_sentence_greedy}\")\n",
    "print(f\"Translated Sentence Multinomial: {translated_sentence_multinomial}\")\n",
    "print(f\"Translated Sentence Beam: {translated_sentence_beam}\")\n",
    "print(f\"Translated Sentence Beam Multinomial: {translated_sentence_beam_multinomial}\")\n",
    "reference_corpus = val_df['German'].tolist()\n",
    "candidate_corpus = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if len(candidate_corpus) <= 0:\n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(reference_corpus[:10], desc=\"Generating translations\"):\n",
    "            translated_sentence = translate_sentence_multinomial(sentence)\n",
    "            candidate_corpus.append(translated_sentence)\n",
    "\n",
    "    file = open('candidate_corpus.txt', 'w', encoding='utf-8')\n",
    "    for line in candidate_corpus:\n",
    "        file.write(line + \"\\n\\n\")\n",
    "    file.close()\n",
    "candidate_corpus[0], reference_corpus[:len(candidate_corpus)][0]\n",
    "\n",
    "bleu = BLEU()\n",
    "\n",
    "bleu_score = bleu.corpus_score(candidate_corpus, [reference_corpus[:len(candidate_corpus)]])\n",
    "print(f\"BLEU score: {bleu_score.score}\")\n",
    "\n",
    "def calculate_bert_score(candidate_corpus, reference_corpus, batch_size=32):\n",
    "    lowest = min(len(candidate_corpus), len(reference_corpus))\n",
    "    candidate_corpus = candidate_corpus[:lowest]\n",
    "    reference_corpus = reference_corpus[:lowest]\n",
    "\n",
    "    bert_scores = []\n",
    "    num_batches = len(candidate_corpus) // batch_size\n",
    "    for i in tqdm(range(num_batches + 1), desc=\"Calculating BERTScore\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(candidate_corpus))\n",
    "        candidates_batch = candidate_corpus[start_idx:end_idx]\n",
    "        references_batch = reference_corpus[start_idx:end_idx]\n",
    "\n",
    "        _, _, batch_scores = score(candidates_batch, references_batch, lang='en', verbose=False)\n",
    "        bert_scores.extend(batch_scores.tolist())\n",
    "\n",
    "    return bert_scores\n",
    "\n",
    "# Example usage\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "bert_scores = calculate_bert_score(candidate_corpus, reference_corpus)\n",
    "final_bert = sum(bert_scores) / len(bert_scores)\n",
    "print(f\"BERTScore: {final_bert}\")\n",
    "\n",
    "print(\"Scores:\")\n",
    "print(f\"BLEU: {bleu_score.score}\")\n",
    "print(f\"BERT: {final_bert}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9a781-852c-478f-a9fb-3f0eb7252738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
